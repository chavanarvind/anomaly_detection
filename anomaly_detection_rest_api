from flask import Flask, request, jsonify
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

app = Flask(__name__)

# Functions for anomaly detection

def dynamic_data_preprocessing(data, numeric_columns):
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data[numeric_columns])
    return data_scaled

def calculate_silhouette_score(data, labels):
    if len(set(labels)) < 2:
        return -1  # Silhouette score is not defined for one cluster
    return silhouette_score(data, labels)

def tune_isolation_forest(data, n_estimators_list, contamination_list, max_samples_percentage_range=None, max_iterations=10, tolerance=0.01):
    best_score = -1
    best_model = None
    iterations = 0
    previous_score = None

    if max_samples_percentage_range is None:
        max_samples_percentage_range = [0.3, 0.5, 0.7]

    while iterations < max_iterations:
        iterations += 1
        for n_estimators in n_estimators_list:
            for contamination in contamination_list:
                for max_samples_percentage in max_samples_percentage_range:
                    max_samples = max(1, int(len(data) * max_samples_percentage))
                    model = IsolationForest(n_estimators=n_estimators, contamination=contamination,
                                            max_samples=max_samples, random_state=42)
                    labels = model.fit_predict(data)
                    score = calculate_silhouette_score(data, labels)
                    if score > best_score:
                        best_score = score
                        best_model = model
        if previous_score is not None and abs(best_score - previous_score) < tolerance:
            break
        previous_score = best_score
    return best_model

def tune_one_class_svm(data, nu_list, gamma_list, max_iterations=10, tolerance=0.01):
    best_score = -1
    best_model = None
    iterations = 0
    previous_score = None

    while iterations < max_iterations:
        iterations += 1
        for nu in nu_list:
            for gamma in gamma_list:
                model = OneClassSVM(nu=nu, gamma=gamma)
                labels = model.fit_predict(data)
                score = calculate_silhouette_score(data, labels)
                if score > best_score:
                    best_score = score
                    best_model = model
        if previous_score is not None and abs(best_score - previous_score) < tolerance:
            break
        previous_score = best_score
    return best_model

def detect_anomalies(data, window_size, weight_if, weight_svm, max_iterations=10, tolerance=0.01):
    final_labels = np.zeros(data.shape[0])
    silhouette_scores_if = []
    silhouette_scores_svm = []

    for i in range(0, len(data), window_size):
        window = data[i:i + window_size]

        isolation_forest = tune_isolation_forest(window,
                                                 n_estimators_list=[100, 200],
                                                 contamination_list=[0.01, 0.05, 0.1, 0.15],
                                                 max_samples_percentage_range=[0.3, 0.5, 0.7])

        if isolation_forest is not None:
            if_labels = isolation_forest.fit_predict(window)
            if_labels = np.where(if_labels == -1, 1, 0)
        else:
            continue

        if_silhouette = calculate_silhouette_score(window, if_labels)
        silhouette_scores_if.append(if_silhouette)

        svm_model = tune_one_class_svm(window, nu_list=[0.05, 0.1, 0.2], gamma_list=['auto', 0.01, 0.1])
        svm_labels = svm_model.fit_predict(window)
        svm_labels = np.where(svm_labels == -1, 1, 0)

        svm_silhouette = calculate_silhouette_score(window, svm_labels)
        silhouette_scores_svm.append(svm_silhouette)

        combined_labels = (weight_if * if_labels + weight_svm * svm_labels) / (weight_if + weight_svm)
        final_labels[i:i + window_size] = np.where(combined_labels > 0.6, 1, 0)
    return final_labels, silhouette_scores_if, silhouette_scores_svm

# Define the API endpoint
@app.route('/detect_anomalies', methods=['POST'])
def detect_anomalies_api():
    data = request.json

    # Get parameters from request or set defaults
    window_size = data.get("window_size", 9406)
    weight_if = data.get("weight_if", 0.6)
    weight_svm = data.get("weight_svm", 0.5)

    # Load and preprocess data
    file_path = 'path/to/your/data.csv'
    data_df = pd.read_csv(file_path, sep=';', parse_dates=['datetime'])
    numeric_columns = data_df.select_dtypes(include=[np.number]).columns.tolist()
    data_scaled = dynamic_data_preprocessing(data_df, numeric_columns)
    dates = data_df['datetime'].values

    # Run anomaly detection
    final_labels, silhouette_scores_if, silhouette_scores_svm = detect_anomalies(data_scaled, window_size, weight_if, weight_svm)

    # Prepare and return the response
    results = {
        "final_labels": final_labels.tolist(),
        "silhouette_scores_if": silhouette_scores_if,
        "silhouette_scores_svm": silhouette_scores_svm
    }
    return jsonify(results)

if __name__ == '__main__':
    app.run(debug=True)
