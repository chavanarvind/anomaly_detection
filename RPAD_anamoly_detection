import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.metrics import silhouette_score
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt

# Step 1: Remove low-variance columns
def remove_low_variance_columns(df, threshold=0.01):
    low_variance_cols = df.columns[df.var() < threshold]
    return df.drop(columns=low_variance_cols)

# Step 2: Create features from sliding windows
def create_features_and_scores(df, window_size):
    features = []
    for i in range(len(df) - window_size + 1):
        window = df.iloc[i:i + window_size]
        mean_features = window.mean().to_list()  # Mean of the window
        features.append(mean_features)
    return pd.DataFrame(features)

# Step 3: Calculate Silhouette Score using features and anomaly labels
def calculate_silhouette_score(feature_df, anomaly_labels):
    if len(set(anomaly_labels)) > 1:  # Requires at least 2 clusters (anomaly, normal)
        score = silhouette_score(feature_df, anomaly_labels)
        return score
    return np.nan

# Step 4: Tune hyperparameters using GridSearchCV
def tune_hyperparameters(feature_df):
    param_grid = {
        'contamination': [0.01, 0.05, 0.1],
        'max_samples': [0.5, 0.75, 1.0],
        'n_estimators': [50, 100, 200],
        'max_features': [1.0, 0.5]
    }
    
    iso_forest = IsolationForest(random_state=42)
    grid_search = GridSearchCV(iso_forest, param_grid, scoring='neg_mean_squared_error', cv=5)
    grid_search.fit(feature_df)
    
    return grid_search.best_estimator_

# Step 5: Detect anomalies with feedback using Silhouette Score
def detect_anomalies_with_feedback(df, window_size, timestamp_col, max_iterations=5):
    # Remove low-variance columns
    df = remove_low_variance_columns(df)

    # Sort by timestamp
    df = df.sort_values(by=timestamp_col)

    # Create features from sliding windows
    feature_df = create_features_and_scores(df, window_size)

    # Tune hyperparameters using GridSearchCV
    best_model = tune_hyperparameters(feature_df)

    # Initialize variables for feedback loop
    best_silhouette_score = -1
    iteration = 0

    while iteration < max_iterations:
        # Apply the Isolation Forest model for anomaly detection
        anomaly_labels = best_model.fit_predict(feature_df)

        # Calculate Silhouette Score
        overall_score = calculate_silhouette_score(feature_df, anomaly_labels)
        print(f"Overall Silhouette Score after iteration {iteration + 1}: {overall_score}")

        # Check for improvement
        if overall_score > best_silhouette_score:
            best_silhouette_score = overall_score
            print("Improvement detected.")
        else:
            print("No improvement, adjusting parameters...")
            # Adjust contamination parameter if necessary
            best_model.set_params(contamination=max(0.01, best_model.contamination - 0.01))

        iteration += 1

    # Final results
    anomalies = feature_df[anomaly_labels == -1]
    return anomalies, feature_df

# Step 6: Visualize results with dynamic graph
def plot_anomalies(df, anomalies, value_col, timestamp_col):
    plt.figure(figsize=(12, 6))
    plt.plot(df[timestamp_col], df[value_col], label=value_col)
    plt.scatter(anomalies.index, anomalies[value_col], color='red', label='Anomalies', zorder=5)
    plt.title('Anomaly Detection in Time Series Data')
    plt.xlabel('Time')
    plt.ylabel('Values')
    plt.legend()
    plt.show()

# Example Data
df = pd.DataFrame({
    'timestamp': pd.date_range(start='2023-01-01', periods=100),
    'value1': np.random.randn(100),
    'value2': np.random.randn(100)
})

# Run anomaly detection
window_size = 10
timestamp_col = 'timestamp'
anomalies, feature_df = detect_anomalies_with_feedback(df, window_size, timestamp_col)

# Visualize anomalies
plot_anomalies(df, anomalies, 'value1', timestamp_col)
